{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62ff2d4-c94f-4533-8ac3-b5efde777251",
   "metadata": {},
   "source": [
    "## Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a7cd8c-2962-4684-ae49-91e7c14a5059",
   "metadata": {},
   "source": [
    "### Overfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset. Because of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model. The overfitted model has low bias and high variance.\n",
    "### Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b30a5c-509a-428a-b1c6-200b5b13e36a",
   "metadata": {},
   "source": [
    "## Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b88b8-339b-4761-9c71-2637c7d5a051",
   "metadata": {},
   "source": [
    "### 1. Hold-out (data):- Rather than using all of our data for training, we can simply split our dataset into two sets: training and testing. A common split ratio is 80% for training and 20% for testing. \n",
    "### 2. Data augmentation (data):- A larger dataset would reduce overfitting. If we cannot gather more data and are constrained to the data we have in our current dataset, we can apply data augmentation to artificially increase the size of our dataset.\n",
    "### 3. Feature selection (data):- If we have only a limited amount of training samples, each with a large number of features, we should only select the most important features for training so that our model doesn’t need to learn for so many features and eventually overfit. \n",
    "### 4. L1 / L2 regularization (learning algorithm):- Regularization is a technique to constrain our network from learning a model that is too complex, which may therefore overfit. In L1 or L2 regularization, we can add a penalty term on the cost function to push the estimated coefficients towards zero (and not take more extreme values). L2 regularization allows weights to decay towards zero but not to zero, while L1 regularization allows weights to decay to zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4148b-f0dd-40b4-86c5-b0e482a49305",
   "metadata": {},
   "source": [
    "## Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdba50d-8f67-4450-90b0-0707a63630cb",
   "metadata": {},
   "source": [
    "### Its occurrence simply means that our model or the algorithm does not fit the data well enough. It usually happens when we have less data to build an accurate model and also when we try to build a linear model with fewer non-linear data.\n",
    "### Scenerios:\n",
    "### High bias and low variance.\n",
    "### The size of the training dataset used is not enough.\n",
    "### The model is too simple.\n",
    "### Training data is not cleaned and also contains noise in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa16974-40ca-45d7-8a61-7f84c6a86133",
   "metadata": {},
   "source": [
    "## Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d769a-d733-4ee6-acd1-3f8433e90a93",
   "metadata": {},
   "source": [
    "### If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data. This tradeoff in complexity is why there is a tradeoff between bias and variance.\n",
    "### Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance. When a data engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance. This way, the model will fit with the data set while increasing the chances of inaccurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e408a237-a358-4f67-9857-4bd8d6fb2854",
   "metadata": {},
   "source": [
    "## Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c7d958-0dd3-40f8-8c58-57aeb4ff59c0",
   "metadata": {},
   "source": [
    "### A model under fits when it is too simple with regards to the data it is trying to model.\n",
    "### If our model does much better on the training set than on the test set, then we’re likely overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa1d1e-e5df-4475-b81c-3a42218bcb5c",
   "metadata": {},
   "source": [
    "## Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c138685-b3ed-4191-ad0b-4230b7a37453",
   "metadata": {},
   "source": [
    "### Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance. When a data engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance. This way, the model will fit with the data set while increasing the chances of inaccurate predictions. The same applies when creating a low variance model with a higher bias. While it will reduce the risk of inaccurate predictions, the model will not properly match the data set. It’s a delicate balance between these bias and variance. Importantly, however, having a higher variance does not indicate a bad ML algorithm. Machine learning algorithms should be able to handle some variance.\n",
    "###  Linear Regression, Linear Discriminant Analysis and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9185cc8-fd1c-4406-9052-81609063f8f5",
   "metadata": {},
   "source": [
    "## Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73ddbd-94fd-40fa-bfd0-156810d10b27",
   "metadata": {},
   "source": [
    "### Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n",
    "### 1. Modify loss function:- In these regularization techniques, the loss function under which the model is optimized is modified to directly take into account the norm of the learned parameters or the output distribution.\n",
    "### 2. L2 regularization, we modify the loss to include the weighted L2 norm of the weights (beta) being optimized. This prevents the weights from getting too large and hence avoiding them to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e7803-d81e-4231-82ac-aa44cc750f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
